{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "name": "Sentiment_Analysis_IMDb_Part_2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "YI1syOqAL9Dy"
      ],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlLmK3frL9Dh",
        "outputId": "f57e3aed-008b-4a1d-ad8e-23dc62cbf4ec"
      },
      "source": [
        "import urllib.request\n",
        "import os\n",
        "import tarfile  \n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence \n",
        "from keras.preprocessing.text import Tokenizer \n",
        "import re\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "from keras import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy.random import RandomState\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn import  linear_model\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn import  linear_model\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import preprocessing\n",
        "from scipy.stats import laplace\n",
        "from math import log\n",
        "from random import random\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import binarize\n",
        "import itertools\n",
        "import random\n",
        "from scipy.optimize import minimize\n",
        "import csv\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc\n",
        "import operator\n",
        "from random import seed\n",
        "from random import randrange\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
        "from sklearn.metrics import log_loss\n",
        "from keras.layers.core import Dense, Dropout, Activation,Flatten"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jdd8-T-ZL9Dp"
      },
      "source": [
        "# Part 1 Keras Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qBsBkl_L9Dq",
        "outputId": "f1d9004e-59a7-4da5-cd26-ed4d70baa5a6"
      },
      "source": [
        "vocabulary_size = 5000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size)\n",
        "\n",
        "print('Loaded dataset with {} training samples, {} test samples'.format(len(X_train), len(X_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded dataset with 25000 training samples, 25000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6NVn6l8L9Dq",
        "outputId": "93fb8b02-d87b-4bbf-ffaf-3d067f011a9c"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4HnNOjpL9Dr",
        "outputId": "8292d9ac-34db-4819-ccae-e06ba3c08001"
      },
      "source": [
        "print('---review---')\n",
        "print(X_train[6])\n",
        "print('---label---')\n",
        "print(y_train[6])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---review---\n",
            "[1, 2, 365, 1234, 5, 1156, 354, 11, 14, 2, 2, 7, 1016, 2, 2, 356, 44, 4, 1349, 500, 746, 5, 200, 4, 4132, 11, 2, 2, 1117, 1831, 2, 5, 4831, 26, 6, 2, 4183, 17, 369, 37, 215, 1345, 143, 2, 5, 1838, 8, 1974, 15, 36, 119, 257, 85, 52, 486, 9, 6, 2, 2, 63, 271, 6, 196, 96, 949, 4121, 4, 2, 7, 4, 2212, 2436, 819, 63, 47, 77, 2, 180, 6, 227, 11, 94, 2494, 2, 13, 423, 4, 168, 7, 4, 22, 5, 89, 665, 71, 270, 56, 5, 13, 197, 12, 161, 2, 99, 76, 23, 2, 7, 419, 665, 40, 91, 85, 108, 7, 4, 2084, 5, 4773, 81, 55, 52, 1901]\n",
            "---label---\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmE1U2_wL9Dr",
        "outputId": "6b091a6c-9942-4c30-fe28-96152d429353"
      },
      "source": [
        "word2id = imdb.get_word_index()\n",
        "id2word = {i: word for word, i in word2id.items()}\n",
        "print('---review with words---')\n",
        "print([id2word.get(i, ' ') for i in X_train[6]])\n",
        "print('---label---')\n",
        "print(y_train[6])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---review with words---\n",
            "['the', 'and', 'full', 'involving', 'to', 'impressive', 'boring', 'this', 'as', 'and', 'and', 'br', 'villain', 'and', 'and', 'need', 'has', 'of', 'costumes', 'b', 'message', 'to', 'may', 'of', 'props', 'this', 'and', 'and', 'concept', 'issue', 'and', 'to', \"god's\", 'he', 'is', 'and', 'unfolds', 'movie', 'women', 'like', \"isn't\", 'surely', \"i'm\", 'and', 'to', 'toward', 'in', \"here's\", 'for', 'from', 'did', 'having', 'because', 'very', 'quality', 'it', 'is', 'and', 'and', 'really', 'book', 'is', 'both', 'too', 'worked', 'carl', 'of', 'and', 'br', 'of', 'reviewer', 'closer', 'figure', 'really', 'there', 'will', 'and', 'things', 'is', 'far', 'this', 'make', 'mistakes', 'and', 'was', \"couldn't\", 'of', 'few', 'br', 'of', 'you', 'to', \"don't\", 'female', 'than', 'place', 'she', 'to', 'was', 'between', 'that', 'nothing', 'and', 'movies', 'get', 'are', 'and', 'br', 'yes', 'female', 'just', 'its', 'because', 'many', 'br', 'of', 'overly', 'to', 'descent', 'people', 'time', 'very', 'bland']\n",
            "---label---\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo52_c2JL9Dr"
      },
      "source": [
        "# word2id"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8l4Je5ML9Ds",
        "outputId": "36b5a695-d497-44ea-dea2-68b874d1c192"
      },
      "source": [
        "print('Maximumreview length: {}'.format(\n",
        "    len(max((X_train+ X_test), key=len))))\n",
        "print('Minimum review length: {}'.format(\n",
        "len(min((X_test + X_test), key=len))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximumreview length: 2697\n",
            "Minimum review length: 14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLaZP0k8L9Ds",
        "outputId": "514d8d02-d462-41af-b75c-2f2eba2f7277"
      },
      "source": [
        "max_words = 1000\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
        "len(X_train),len(X_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 25000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_A9DxP32L9Dt",
        "outputId": "1208e5d5-0178-4ffe-a341-902e7a0c9448"
      },
      "source": [
        "embedding_size=32\n",
        "model=Sequential()\n",
        "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /Users/pan/.local/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 1000, 32)          160000    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 213,301\n",
            "Trainable params: 213,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YP-PdJoL9Dt"
      },
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxWzW8XEL9Dt",
        "outputId": "9c20c610-db83-412d-dbdf-1482f62e0cd9"
      },
      "source": [
        "batch_size = 64\n",
        "num_epochs = 3\n",
        "X_valid, y_valid = X_train[:batch_size], y_train[:batch_size]\n",
        "X_train2, y_train2 = X_train[batch_size:], y_train[batch_size:]\n",
        "model.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), \n",
        "        batch_size= batch_size, epochs=num_epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /Users/pan/.local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 24936 samples, validate on 64 samples\n",
            "Epoch 1/3\n",
            "24936/24936 [==============================] - 260s 10ms/step - loss: 0.4742 - accuracy: 0.7614 - val_loss: 0.2300 - val_accuracy: 0.9531\n",
            "Epoch 2/3\n",
            "24936/24936 [==============================] - 268s 11ms/step - loss: 0.3245 - accuracy: 0.8651 - val_loss: 0.2127 - val_accuracy: 0.9531\n",
            "Epoch 3/3\n",
            "24936/24936 [==============================] - 266s 11ms/step - loss: 0.2475 - accuracy: 0.9028 - val_loss: 0.2255 - val_accuracy: 0.9531\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x217a804c88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gR9J0Y8LL9Dt",
        "outputId": "3d8a26a1-988c-4ea1-f8c8-9862eac7b651"
      },
      "source": [
        "scores =model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test accuracy:', scores[1])\n",
        "scores"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.8734800219535828\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.30487914228439333, 0.8734800219535828]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 290
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_-OnBkKL9Du"
      },
      "source": [
        "### Second model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDq9hRq9L9Du",
        "outputId": "aa1c2a1e-4864-4fcc-d17e-39de38c9fb71"
      },
      "source": [
        "model_2 = Sequential()\n",
        "model_2.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
        "model_2.add(Dropout(0.2))  \n",
        "model_2.add(Flatten()) \n",
        "model_2.add(Dense(units=256,\n",
        "                activation='relu' ))\n",
        "model_2.add(Dropout(0.2))\n",
        "model_2.add(Dense(units=1,\n",
        "                activation='sigmoid' )) \n",
        "model_2.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 1000, 32)          160000    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1000, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 32000)             0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 256)               8192256   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 8,352,513\n",
            "Trainable params: 8,352,513\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rePgaOQuL9Du"
      },
      "source": [
        "model_2.compile(loss='binary_crossentropy', \n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKNhIFo-L9Dv",
        "outputId": "03ae2125-0c64-4000-80db-54da23e333d4"
      },
      "source": [
        "train_history =model_2.fit(X_train2, y_train2,batch_size= batch_size, \n",
        "                         epochs=num_epochs,verbose=2,\n",
        "                         validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 19948 samples, validate on 4988 samples\n",
            "Epoch 1/3\n",
            " - 51s - loss: 0.0155 - accuracy: 0.9951 - val_loss: 0.7769 - val_accuracy: 0.8502\n",
            "Epoch 2/3\n",
            " - 50s - loss: 0.0120 - accuracy: 0.9956 - val_loss: 0.9122 - val_accuracy: 0.8571\n",
            "Epoch 3/3\n",
            " - 48s - loss: 0.0088 - accuracy: 0.9970 - val_loss: 0.8855 - val_accuracy: 0.8583\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ETqNNUEL9Dv",
        "outputId": "c5afdab8-43ff-4d00-d02a-00610cb59c45"
      },
      "source": [
        "scores_2 = model_2.evaluate(X_test, y_test, verbose=1)\n",
        "scores_2[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 5s 204us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8586000204086304"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 311
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQZKHvM7L9Dv"
      },
      "source": [
        "probility=model_2.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsFYFY6TL9Dw"
      },
      "source": [
        "predict=model_2.predict_classes(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2bT80UcL9Dw",
        "outputId": "16c4051f-8b3e-4a0c-c424-f482b5ac627d"
      },
      "source": [
        "predict_classes=predict.reshape(-1)\n",
        "predict_classes[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 0, 0, 1, 1, 1, 0, 1, 1], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6gHgwj-L9Dx"
      },
      "source": [
        "SentimentDict={1:'POS',0:'NEG'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnoZCciUL9Dx"
      },
      "source": [
        "def display_test_Sentiment(i):\n",
        "    print(X_test[i])\n",
        "    print('label:',SentimentDict[y_test[i]],\n",
        "          'PREDIXTION:',SentimentDict[predict_classes[i]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mc8WOH7_L9Dx",
        "outputId": "eb9afab2-cb52-483d-afaf-0eb98fa66af7"
      },
      "source": [
        "display_test_Sentiment(24000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    1   50   71   64\n",
            "  107 1653  944   44   14   20    4  936    7    2    5   94    2    5\n",
            "    4   96   36 2399    4 4338   39  406  812    8 3926  812  859   44\n",
            "    4  114   42  769   39    4  274  507  340 4948   39   14   20   11\n",
            "  192   44    4   64  183 2937  120   39    4  274   26    4 1433    7\n",
            "    4  293 1850   60   95  490 1201 2537    4    2   48    4   22 1188\n",
            "   69   93    6   52   20   60  151    2    8    4  274   13  586   28\n",
            "   77   38  685  472   36  122   24    4  114    5  769   26  801    7\n",
            "  364  352  189 1554    4  116    9 1639    5    4 3667 2048 4885  449\n",
            "  422    4   96    4    2    8   68 2121    9 1050    2   13 1408    6\n",
            "    2    7 4637    9  688   18    4 1626    2    4    2   71  307 1389\n",
            "    5   73 1319   11   68  555]\n",
            "label: NEG PREDIXTION: NEG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YI1syOqAL9Dy"
      },
      "source": [
        "#### We can see dropout having the desired impact on training with a slightly slower trend in convergence and in this case a lower final accuracy. The model could probably use a few more epochs of training and may achieve a higher skill (try it an see)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dY0JN3mdL9Dy"
      },
      "source": [
        "# Part 2 LabeledTrainData"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sau60xdxL9Dy"
      },
      "source": [
        "train = pd.read_csv('/Users/pan/Desktop/labeledTrainData.tsv', sep='\\t')\n",
        "test = pd.read_csv('/Users/pan/Desktop/testData.tsv', sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVxrpj_9L9Dy",
        "outputId": "241e2ff9-c7c2-4774-8278-23c66135d44c"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5814_8</td>\n",
              "      <td>1</td>\n",
              "      <td>With all this stuff going down at the moment w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2381_9</td>\n",
              "      <td>1</td>\n",
              "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7759_3</td>\n",
              "      <td>0</td>\n",
              "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3630_4</td>\n",
              "      <td>0</td>\n",
              "      <td>It must be assumed that those who praised this...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9495_8</td>\n",
              "      <td>1</td>\n",
              "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       id  sentiment                                             review\n",
              "0  5814_8          1  With all this stuff going down at the moment w...\n",
              "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
              "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
              "3  3630_4          0  It must be assumed that those who praised this...\n",
              "4  9495_8          1  Superbly trashy and wondrously unpretentious 8..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NEyTxrFL9Dz",
        "outputId": "7129902e-6b28-48b4-eb31-f8ac7d86a858"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12311_10</td>\n",
              "      <td>Naturally in a film who's main themes are of m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8348_2</td>\n",
              "      <td>This movie is a disaster within a disaster fil...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5828_4</td>\n",
              "      <td>All in all, this is a movie for kids. We saw i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7186_2</td>\n",
              "      <td>Afraid of the Dark left me with the impression...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>12128_7</td>\n",
              "      <td>A very accurate depiction of small time mob li...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id                                             review\n",
              "0  12311_10  Naturally in a film who's main themes are of m...\n",
              "1    8348_2  This movie is a disaster within a disaster fil...\n",
              "2    5828_4  All in all, this is a movie for kids. We saw i...\n",
              "3    7186_2  Afraid of the Dark left me with the impression...\n",
              "4   12128_7  A very accurate depiction of small time mob li..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eYeVjRPL9Dz",
        "outputId": "ec2b6f73-45ec-4c03-bab2-6408dde720df"
      },
      "source": [
        "train.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id           25000\n",
              "sentiment    25000\n",
              "review       25000\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee_cZf90L9Dz",
        "outputId": "a6b1ea50-387c-4c4e-a196-8e4c907c2633"
      },
      "source": [
        "test.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id        25000\n",
              "review    25000\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keEAYUBtL9Dz"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCZqnciIL9D0"
      },
      "source": [
        "def review_to_text(review, remove_stopwords):\n",
        "    #remove html\n",
        "    raw_text = BeautifulSoup(review, 'html.parser').get_text()\n",
        "    letters = re.sub('[^a-zA-Z]', ' ', raw_text)\n",
        "    words = letters.lower().split()\n",
        "    #remove stop words\n",
        "    if remove_stopwords:\n",
        "        all_stop_words = set(stopwords.words('english'))\n",
        "        words = [w for w in words if w not in all_stop_words]\n",
        "    return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3VXModWL9D0",
        "outputId": "fd991e49-d894-472c-a66d-6ab08e1e8d26"
      },
      "source": [
        "X_train_2 = [' '.join(review_to_text(review, True)) for review in train['review']]\n",
        "X_train_2[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'stuff going moment mj started listening music watching odd documentary watched wiz watched moonwalker maybe want get certain insight guy thought really cool eighties maybe make mind whether guilty innocent moonwalker part biography part feature film remember going see cinema originally released subtle messages mj feeling towards press also obvious message drugs bad kay visually impressive course michael jackson unless remotely like mj anyway going hate find boring may call mj egotist consenting making movie mj fans would say made fans true really nice actual feature film bit finally starts minutes excluding smooth criminal sequence joe pesci convincing psychopathic powerful drug lord wants mj dead bad beyond mj overheard plans nah joe pesci character ranted wanted people know supplying drugs etc dunno maybe hates mj music lots cool things like mj turning car robot whole speed demon sequence also director must patience saint came filming kiddy bad sequence usually directors hate working one kid let alone whole bunch performing complex dance scene bottom line movie people like mj one level another think people stay away try give wholesome message ironically mj bestest buddy movie girl michael jackson truly one talented people ever grace planet guilty well attention gave subject hmmm well know people different behind closed doors know fact either extremely nice stupid guy one sickest liars hope latter'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8orwX-qL9D0"
      },
      "source": [
        "X_test_2 = [' '.join(review_to_text(review, True)) for review in test['review']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwG335hoL9D0",
        "outputId": "ac964454-ef58-4760-d5c4-f998bfb9f7d4"
      },
      "source": [
        "y_train_2 = train['sentiment']\n",
        "y_train_2.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVo9JsIJL9D0"
      },
      "source": [
        "## Feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2P1J4kZL9D1"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tm7ABuZnL9D1"
      },
      "source": [
        "### Countvec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEwepkYCL9D1"
      },
      "source": [
        "count_vec = CountVectorizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2orYI1XYL9D1"
      },
      "source": [
        "X_count = count_vec.fit_transform(X_train_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeJ4x-y4L9D1",
        "outputId": "cdcbd518-ee06-403b-80c9-c2bd147e87e0"
      },
      "source": [
        "count_vec.get_feature_names()[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['aa',\n",
              " 'aaa',\n",
              " 'aaaaaaah',\n",
              " 'aaaaah',\n",
              " 'aaaaatch',\n",
              " 'aaaahhhhhhh',\n",
              " 'aaaand',\n",
              " 'aaaarrgh',\n",
              " 'aaah',\n",
              " 'aaargh']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZA5YSBMXL9D1",
        "outputId": "44248bfd-f990-480b-ab0c-28f12cf4022e"
      },
      "source": [
        "X_count.toarray()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5lWCVL1L9D2"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMtPXZA0L9D2"
      },
      "source": [
        "tfidf_vec = TfidfVectorizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mdl4FkCL9D2"
      },
      "source": [
        "X_tfidf = tfidf_vec.fit_transform(X_train_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yAyRQ21L9D2",
        "outputId": "451fde61-1c58-4184-db20-ca2f8a4211b7"
      },
      "source": [
        "print(X_tfidf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 62941)\t0.03354045373162439\n",
            "  (0, 26968)\t0.07217307173962342\n",
            "  (0, 42578)\t0.034091761849276245\n",
            "  (0, 42350)\t0.7703186447281194\n",
            "  (0, 62043)\t0.03487297244743932\n",
            "  (0, 38035)\t0.04751617903415886\n",
            "  (0, 43600)\t0.054295467513819196\n",
            "  (0, 71488)\t0.023073898100782347\n",
            "  (0, 45703)\t0.03926522998399744\n",
            "  (0, 18360)\t0.03764384002495705\n",
            "  (0, 71482)\t0.05675846026639458\n",
            "  (0, 72628)\t0.07163890269512017\n",
            "  (0, 42819)\t0.13307302119828857\n",
            "  (0, 40560)\t0.08512551931849857\n",
            "  (0, 71313)\t0.024749737549733447\n",
            "  (0, 26303)\t0.018496729631339144\n",
            "  (0, 10346)\t0.03669746300625778\n",
            "  (0, 32847)\t0.047604829337073856\n",
            "  (0, 28243)\t0.054847839906128716\n",
            "  (0, 65818)\t0.02525032488112832\n",
            "  (0, 52926)\t0.03509094232335691\n",
            "  (0, 13681)\t0.07064593076344598\n",
            "  (0, 20094)\t0.053346084856950304\n",
            "  (0, 39462)\t0.019295315365876368\n",
            "  (0, 41891)\t0.02908726933031267\n",
            "  :\t:\n",
            "  (24999, 11024)\t0.2336985200945862\n",
            "  (24999, 69731)\t0.0694608436759633\n",
            "  (24999, 56821)\t0.0789407453282018\n",
            "  (24999, 8212)\t0.10419563187936105\n",
            "  (24999, 66496)\t0.12765702217937216\n",
            "  (24999, 14829)\t0.09737592096988773\n",
            "  (24999, 40865)\t0.09289618777845667\n",
            "  (24999, 35609)\t0.09281349488722045\n",
            "  (24999, 1068)\t0.07846017144128777\n",
            "  (24999, 27650)\t0.10427216130696523\n",
            "  (24999, 64682)\t0.10681691349121074\n",
            "  (24999, 2372)\t0.11553867789106884\n",
            "  (24999, 68807)\t0.14217487881642793\n",
            "  (24999, 11148)\t0.11127402971425696\n",
            "  (24999, 23000)\t0.1344834729529056\n",
            "  (24999, 8208)\t0.11379000146648559\n",
            "  (24999, 16662)\t0.09200629977265856\n",
            "  (24999, 14778)\t0.12738720964502964\n",
            "  (24999, 16210)\t0.1543987333647921\n",
            "  (24999, 36598)\t0.15222188728065744\n",
            "  (24999, 60728)\t0.12738720964502964\n",
            "  (24999, 68690)\t0.15122262606917364\n",
            "  (24999, 33308)\t0.17252625322981072\n",
            "  (24999, 11104)\t0.37067376366906934\n",
            "  (24999, 68783)\t0.19283061917896396\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrjcf4yyL9D2",
        "outputId": "0873f7a3-24c0-4370-fd21-11e77c83e398"
      },
      "source": [
        "X_tfidf.toarray()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WueXLM2FL9D3"
      },
      "source": [
        "## Multiple Model Prediciton\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eluoGmnoL9D3"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SlyuFoJL9D3"
      },
      "source": [
        "X1_train = X_train_2[:20000]\n",
        "X1_test = X_train_2[20000:]\n",
        "y1_train = y_train_2[:20000]\n",
        "y1_test = y_train_2[20000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0aS1ZDNL9D3",
        "outputId": "f0bad2be-d0d9-497e-d845-de20f5e2da31"
      },
      "source": [
        "len(X1_train), len(X1_test), len(y1_train), len(y1_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20000, 5000, 20000, 5000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBqf1TXML9D3"
      },
      "source": [
        "### MultinomialNB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRRUFaRhL9D4"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-O1FMQAL9D4"
      },
      "source": [
        "def MNB_count_Classifier():              \n",
        "    return Pipeline([\n",
        "        ('count_vec', CountVectorizer()),  \n",
        "        ('mnb', MultinomialNB())       \n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeL1s-gbL9D4",
        "outputId": "c6d424cb-768d-4084-edb3-9791eaf3f4ba"
      },
      "source": [
        "mnbc_clf = MNB_count_Classifier()\n",
        "mnbc_clf.fit(X1_train, y1_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('count_vec',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 1), preprocessor=None,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, vocabulary=None)),\n",
              "                ('mnb',\n",
              "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMMmKbDnL9D4",
        "outputId": "856c1186-281b-415b-fda6-2d75466113a5"
      },
      "source": [
        "mnbc_clf.score(X1_test, y1_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8534"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGgYCJA3L9D4"
      },
      "source": [
        "def MNB_tfidf_Classifier():              \n",
        "    return Pipeline([\n",
        "        ('tfidf_vec', TfidfVectorizer()),  \n",
        "        ('mnb', MultinomialNB())       \n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f63SqgiKL9D5",
        "outputId": "f456114c-3642-4279-c288-e04a12769db7"
      },
      "source": [
        "mnbt_clf = MNB_tfidf_Classifier()\n",
        "mnbt_clf.fit(X1_train, y1_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidf_vec',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('mnb',\n",
              "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NSeh6DDL9D5",
        "outputId": "219a2d7d-c9aa-4036-da4d-06b1077d14d3"
      },
      "source": [
        "mnbt_clf.score(X1_test, y1_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8598"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0_1vh3jL9D5"
      },
      "source": [
        "### Logistics regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ow6HypFlL9D5"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmwhLRb8L9D5"
      },
      "source": [
        "def LogisticRegression_c():              \n",
        "    return Pipeline([\n",
        "        ('count_vec', CountVectorizer()),  \n",
        "#         ('poly', PolynomialFeatures(degree=degree)),            \n",
        "        ('logistic', LogisticRegression(C=0.1, penalty='l2'))     \n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FT1afy2jL9D6",
        "outputId": "6711df27-6146-4625-8b71-2bcac2d21e89"
      },
      "source": [
        "polyc_log_reg = LogisticRegression_c()\n",
        "polyc_log_reg.fit(X1_train, y1_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/Users/pan/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('count_vec',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 1), preprocessor=None,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, vocabulary=None)),\n",
              "                ('logistic',\n",
              "                 LogisticRegression(C=0.1, class_weight=None, dual=False,\n",
              "                                    fit_intercept=True, intercept_scaling=1,\n",
              "                                    l1_ratio=None, max_iter=100,\n",
              "                                    multi_class='warn', n_jobs=None,\n",
              "                                    penalty='l2', random_state=None,\n",
              "                                    solver='warn', tol=0.0001, verbose=0,\n",
              "                                    warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYzv0bz0L9D6",
        "outputId": "ea8b4ac7-05a9-4bb3-a66f-2b6f9ba9964b"
      },
      "source": [
        "polyc_log_reg.score(X1_test, y1_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8806"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nM0jLkQ3L9D6"
      },
      "source": [
        "def LogisticRegression_t():              \n",
        "    return Pipeline([\n",
        "        ('tfidf_vec', TfidfVectorizer()),  \n",
        "#         ('poly', PolynomialFeatures(degree=degree)),            \n",
        "        ('logistic', LogisticRegression(C=0.1, penalty='l2'))     \n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utup0JooL9D6",
        "outputId": "7f5ef438-da07-45b6-9411-2dda9ef8f992"
      },
      "source": [
        "polyt_log_reg = LogisticRegression_t()\n",
        "polyt_log_reg.fit(X1_train, y1_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidf_vec',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('logistic',\n",
              "                 LogisticRegression(C=0.1, class_weight=None, dual=False,\n",
              "                                    fit_intercept=True, intercept_scaling=1,\n",
              "                                    l1_ratio=None, max_iter=100,\n",
              "                                    multi_class='warn', n_jobs=None,\n",
              "                                    penalty='l2', random_state=None,\n",
              "                                    solver='warn', tol=0.0001, verbose=0,\n",
              "                                    warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NBApmtAL9D7",
        "outputId": "93bd3a88-870e-4e5b-8563-92c869697b4f"
      },
      "source": [
        "polyt_log_reg.score(X1_test, y1_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8606"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SABP4BvL9D7"
      },
      "source": [
        "### rbfSVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlEen0qHL9D7"
      },
      "source": [
        "from sklearn.svm import SVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgZnlmaPL9D8"
      },
      "source": [
        "def RBFKernelSVC_c(gamma=1.0):\n",
        "    return Pipeline([\n",
        "        ('count_vec', CountVectorizer()),  \n",
        "        (\"svc\", SVC(kernel=\"rbf\", gamma=gamma)) \n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1Qc9VVxL9D8",
        "outputId": "933b1a1a-5338-4574-add8-cca50deab9e5"
      },
      "source": [
        "svcc_clf = RBFKernelSVC_c()\n",
        "svcc_clf.fit(X1_train, y1_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('count_vec',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 1), preprocessor=None,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, vocabulary=None)),\n",
              "                ('svc',\n",
              "                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
              "                     decision_function_shape='ovr', degree=3, gamma=1.0,\n",
              "                     kernel='rbf', max_iter=-1, probability=False,\n",
              "                     random_state=None, shrinking=True, tol=0.001,\n",
              "                     verbose=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 286
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5LQN9JUL9D8",
        "outputId": "487a13f4-7c90-48f9-9459-499fae98da0d"
      },
      "source": [
        "svcc_clf.score(X1_test, y1_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.497"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 287
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c0m2DNAL9D8"
      },
      "source": [
        "def RBFKernelSVC_t(gamma=1.0):\n",
        "    return Pipeline([\n",
        "        ('tfidf_vec', TfidfVectorizer()),  \n",
        "        (\"svc\", SVC(kernel=\"rbf\", gamma=gamma)) \n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kgmbKpbL9D9",
        "outputId": "8943ad55-a291-40b0-84c4-3a78955be94f"
      },
      "source": [
        "svct_clf = RBFKernelSVC_t()\n",
        "svct_clf.fit(X1_train, y1_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidf_vec',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('svc',\n",
              "                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
              "                     decision_function_shape='ovr', degree=3, gamma=1.0,\n",
              "                     kernel='rbf', max_iter=-1, probability=False,\n",
              "                     random_state=None, shrinking=True, tol=0.001,\n",
              "                     verbose=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 308
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZU7FFwftL9D9"
      },
      "source": [
        "svct_clf.score(X1_test, y1_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnBqjX0IL9D9"
      },
      "source": [
        "## Ensemble"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbaRzaCqL9D9"
      },
      "source": [
        "### RandomForest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRidXaigL9D9"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MX0xGlxML9D-"
      },
      "source": [
        "def RFC_c():\n",
        "    return Pipeline([\n",
        "        ('count_vec', CountVectorizer()),  \n",
        "        ('rfc', RandomForestClassifier(n_estimators=500, max_depth=3, random_state=666, n_jobs=-1)) \n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KOoYKRIL9D-",
        "outputId": "ad216d48-efd4-4ec0-b17b-3f39639b759c"
      },
      "source": [
        "rfcc_clf = RFC_c()\n",
        "rfcc_clf.fit(X1_train, y1_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('count_vec',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 1), preprocessor=None,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, vocabular...\n",
              "                ('rfc',\n",
              "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
              "                                        criterion='gini', max_depth=3,\n",
              "                                        max_features='auto',\n",
              "                                        max_leaf_nodes=None,\n",
              "                                        min_impurity_decrease=0.0,\n",
              "                                        min_impurity_split=None,\n",
              "                                        min_samples_leaf=1, min_samples_split=2,\n",
              "                                        min_weight_fraction_leaf=0.0,\n",
              "                                        n_estimators=500, n_jobs=-1,\n",
              "                                        oob_score=False, random_state=666,\n",
              "                                        verbose=0, warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYL1ukVYL9D-",
        "outputId": "f13e3a63-c38f-40b2-cff5-d50ce51a0054"
      },
      "source": [
        "rfcc_clf.score(X1_test, y1_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8378"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuRyiTdjL9D-"
      },
      "source": [
        "def RFC_t():\n",
        "    return Pipeline([\n",
        "        ('tfidf_vec', TfidfVectorizer()),  \n",
        "        ('rfc', RandomForestClassifier(n_estimators=500, max_depth=3, random_state=666, n_jobs=-1)) \n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtU23BCwL9D-",
        "outputId": "f86ffba2-9d29-444c-e69d-43ff854841b5"
      },
      "source": [
        "rfct_clf = RFC_t()\n",
        "rfct_clf.fit(X1_train, y1_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidf_vec',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_patte...\n",
              "                ('rfc',\n",
              "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
              "                                        criterion='gini', max_depth=3,\n",
              "                                        max_features='auto',\n",
              "                                        max_leaf_nodes=None,\n",
              "                                        min_impurity_decrease=0.0,\n",
              "                                        min_impurity_split=None,\n",
              "                                        min_samples_leaf=1, min_samples_split=2,\n",
              "                                        min_weight_fraction_leaf=0.0,\n",
              "                                        n_estimators=500, n_jobs=-1,\n",
              "                                        oob_score=False, random_state=666,\n",
              "                                        verbose=0, warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrMZuQYYL9D_",
        "outputId": "87bed446-71e4-490f-b280-2dbee7114a31"
      },
      "source": [
        "rfct_clf.score(X1_test, y1_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8308"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XT-yaKHFL9D_"
      },
      "source": [
        "### ExtraTree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XM0Fny9lL9D_"
      },
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrgAkLrTL9D_"
      },
      "source": [
        "def ETC_c():\n",
        "    return Pipeline([\n",
        "        ('count_vec', CountVectorizer()),  \n",
        "        ('etc', ExtraTreesClassifier(n_estimators=500, bootstrap=True, random_state=666, n_jobs=-1)) \n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-xIgmFJL9D_",
        "outputId": "2daec949-6f54-496d-ee9f-91787246ad69"
      },
      "source": [
        "etc_clf = ETC_c()\n",
        "etc_clf.fit(X1_train, y1_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('count_vec',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 1), preprocessor=None,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, vocabular...\n",
              "                ('etc',\n",
              "                 ExtraTreesClassifier(bootstrap=True, class_weight=None,\n",
              "                                      criterion='gini', max_depth=None,\n",
              "                                      max_features='auto', max_leaf_nodes=None,\n",
              "                                      min_impurity_decrease=0.0,\n",
              "                                      min_impurity_split=None,\n",
              "                                      min_samples_leaf=1, min_samples_split=2,\n",
              "                                      min_weight_fraction_leaf=0.0,\n",
              "                                      n_estimators=500, n_jobs=-1,\n",
              "                                      oob_score=False, random_state=666,\n",
              "                                      verbose=0, warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzYK8sEGL9EA",
        "outputId": "b9fa97fa-bedd-4a12-90ea-5b383c791a93"
      },
      "source": [
        "etc_clf.score(X1_test, y1_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.877"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJ_ue6CGL9EA"
      },
      "source": [
        "def ETC_t():\n",
        "    return Pipeline([\n",
        "        ('tfidf_vec', TfidfVectorizer()),  \n",
        "        ('etc', ExtraTreesClassifier(n_estimators=500, bootstrap=True, random_state=666, n_jobs=-1)) \n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xp86yZXAL9EA",
        "outputId": "fa0ffea7-fa64-4da2-b7ba-9b4bed986724"
      },
      "source": [
        "etct_clf = ETC_t()\n",
        "etct_clf.fit(X1_train, y1_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidf_vec',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_patte...\n",
              "                ('etc',\n",
              "                 ExtraTreesClassifier(bootstrap=True, class_weight=None,\n",
              "                                      criterion='gini', max_depth=None,\n",
              "                                      max_features='auto', max_leaf_nodes=None,\n",
              "                                      min_impurity_decrease=0.0,\n",
              "                                      min_impurity_split=None,\n",
              "                                      min_samples_leaf=1, min_samples_split=2,\n",
              "                                      min_weight_fraction_leaf=0.0,\n",
              "                                      n_estimators=500, n_jobs=-1,\n",
              "                                      oob_score=False, random_state=666,\n",
              "                                      verbose=0, warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KYwDVoiL9EB",
        "outputId": "ade09f55-522f-451c-896d-f69c8088c341"
      },
      "source": [
        "etct_clf.score(X1_test, y1_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.871"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6SfhVevL9EB"
      },
      "source": [
        "### AdaBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_-4NPrrL9EB"
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mpamsEKL9EB"
      },
      "source": [
        "def Ada_c():\n",
        "    return Pipeline([\n",
        "        ('count_vec', CountVectorizer()),  \n",
        "        ('adaboost', AdaBoostClassifier(DecisionTreeClassifier(max_depth=2), n_estimators=500)) \n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gocZC28HL9EB",
        "outputId": "518cccfe-b90f-4acd-8b5b-74cbbd004d4f"
      },
      "source": [
        "adac_clf = Ada_c()\n",
        "adac_clf.fit(X1_train, y1_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('count_vec',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 1), preprocessor=None,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, vocabular...\n",
              "                                    base_estimator=DecisionTreeClassifier(class_weight=None,\n",
              "                                                                          criterion='gini',\n",
              "                                                                          max_depth=2,\n",
              "                                                                          max_features=None,\n",
              "                                                                          max_leaf_nodes=None,\n",
              "                                                                          min_impurity_decrease=0.0,\n",
              "                                                                          min_impurity_split=None,\n",
              "                                                                          min_samples_leaf=1,\n",
              "                                                                          min_samples_split=2,\n",
              "                                                                          min_weight_fraction_leaf=0.0,\n",
              "                                                                          presort=False,\n",
              "                                                                          random_state=None,\n",
              "                                                                          splitter='best'),\n",
              "                                    learning_rate=1.0, n_estimators=500,\n",
              "                                    random_state=None))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwNwio-9L9EC",
        "outputId": "f8127ca4-e748-490b-eaa0-f53be67889c9"
      },
      "source": [
        "adac_clf.score(X1_test, y1_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8424"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5Z2KF3kL9EC"
      },
      "source": [
        "def Ada_t():\n",
        "    return Pipeline([\n",
        "        ('tfidf_vec', TfidfVectorizer()),  \n",
        "        ('adaboost', AdaBoostClassifier(DecisionTreeClassifier(max_depth=2), n_estimators=500)) \n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXhlH8AUL9EC",
        "outputId": "cd03135f-b1bc-404b-ec7c-4a7192c6316a"
      },
      "source": [
        "adat_clf = Ada_t()\n",
        "adat_clf.fit(X1_train, y1_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidf_vec',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_patte...\n",
              "                                    base_estimator=DecisionTreeClassifier(class_weight=None,\n",
              "                                                                          criterion='gini',\n",
              "                                                                          max_depth=2,\n",
              "                                                                          max_features=None,\n",
              "                                                                          max_leaf_nodes=None,\n",
              "                                                                          min_impurity_decrease=0.0,\n",
              "                                                                          min_impurity_split=None,\n",
              "                                                                          min_samples_leaf=1,\n",
              "                                                                          min_samples_split=2,\n",
              "                                                                          min_weight_fraction_leaf=0.0,\n",
              "                                                                          presort=False,\n",
              "                                                                          random_state=None,\n",
              "                                                                          splitter='best'),\n",
              "                                    learning_rate=1.0, n_estimators=500,\n",
              "                                    random_state=None))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRLuGjw7L9EC",
        "outputId": "cc63ad29-9fe6-4a02-8a7e-11f238386217"
      },
      "source": [
        "adat_clf.score(X1_test, y1_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8334"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "304Oty37L9EC"
      },
      "source": [
        "### GBDT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4MlOy5OL9ED"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dP2ES4qL9ED"
      },
      "source": [
        "def GBDT_c():\n",
        "    return Pipeline([\n",
        "        ('count_vec', CountVectorizer()),  \n",
        "        ('GBDT', GradientBoostingClassifier(max_depth=3, n_estimators=50))\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6p3hYgvBL9ED",
        "outputId": "c416b800-7f35-4d87-b3ad-23d56ed904a6"
      },
      "source": [
        "gbtc_clf = GBDT_c()\n",
        "gbtc_clf.fit(X1_train, y1_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('count_vec',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 1), preprocessor=None,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, vocabular...\n",
              "                                            learning_rate=0.1, loss='deviance',\n",
              "                                            max_depth=3, max_features=None,\n",
              "                                            max_leaf_nodes=None,\n",
              "                                            min_impurity_decrease=0.0,\n",
              "                                            min_impurity_split=None,\n",
              "                                            min_samples_leaf=1,\n",
              "                                            min_samples_split=2,\n",
              "                                            min_weight_fraction_leaf=0.0,\n",
              "                                            n_estimators=50,\n",
              "                                            n_iter_no_change=None,\n",
              "                                            presort='auto', random_state=None,\n",
              "                                            subsample=1.0, tol=0.0001,\n",
              "                                            validation_fraction=0.1, verbose=0,\n",
              "                                            warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7T0A7zFL9ED",
        "outputId": "0f5de00b-3648-4106-a6ca-04527cdf25dd"
      },
      "source": [
        "gbtc_clf.score(X1_test, y1_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7778"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRt2BhFOL9EE"
      },
      "source": [
        "def GBDT_t():\n",
        "    return Pipeline([\n",
        "        ('tfidf_vec', TfidfVectorizer()),  \n",
        "        ('GBDT', GradientBoostingClassifier(max_depth=3, n_estimators=50))\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3XT2hGQL9EE",
        "outputId": "c6548cd2-3566-480e-b218-0cc148f26815"
      },
      "source": [
        "gbt_clf = GBDT_t()\n",
        "gbt_clf.fit(X1_train, y1_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidf_vec',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_patte...\n",
              "                                            learning_rate=0.1, loss='deviance',\n",
              "                                            max_depth=3, max_features=None,\n",
              "                                            max_leaf_nodes=None,\n",
              "                                            min_impurity_decrease=0.0,\n",
              "                                            min_impurity_split=None,\n",
              "                                            min_samples_leaf=1,\n",
              "                                            min_samples_split=2,\n",
              "                                            min_weight_fraction_leaf=0.0,\n",
              "                                            n_estimators=50,\n",
              "                                            n_iter_no_change=None,\n",
              "                                            presort='auto', random_state=None,\n",
              "                                            subsample=1.0, tol=0.0001,\n",
              "                                            validation_fraction=0.1, verbose=0,\n",
              "                                            warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHhuAqNdL9EE",
        "outputId": "256f843a-4675-4ba9-9735-c217ac9fca5e"
      },
      "source": [
        "gbt_clf.score(X1_test, y1_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.777"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qz4qE7GCL9EE"
      },
      "source": [
        "### XGboost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6W-WgECL9EF"
      },
      "source": [
        "import xgboost as xgb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_jQEiiJL9EF"
      },
      "source": [
        "def XGB_c():\n",
        "    return Pipeline([\n",
        "        ('count_vec', CountVectorizer()),  \n",
        "        ('XGB', xgb.XGBClassifier(n_estimators=500, max_depth=6, learning_rate=0.1,\n",
        "                                  subsample=.7, colsample_bytree=0.6, gamma=0.05))\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCy-8ajjL9EF",
        "outputId": "2c0e97ec-f3f3-4613-d59d-ee8f3bdbce89"
      },
      "source": [
        "xgbc_clf = XGB_c()\n",
        "xgbc_clf.fit(X1_train, y1_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('count_vec',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 1), preprocessor=None,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, vocabular...\n",
              "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
              "                               colsample_bylevel=1, colsample_bynode=1,\n",
              "                               colsample_bytree=0.6, gamma=0.05,\n",
              "                               learning_rate=0.1, max_delta_step=0, max_depth=6,\n",
              "                               min_child_weight=1, missing=None,\n",
              "                               n_estimators=500, n_jobs=1, nthread=None,\n",
              "                               objective='binary:logistic', random_state=0,\n",
              "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
              "                               seed=None, silent=None, subsample=0.7,\n",
              "                               verbosity=1))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IwuGpKLL9EF",
        "outputId": "797b7efd-af26-4105-a393-ca18fba3ff6c"
      },
      "source": [
        "xgbc_clf.score(X1_test, y1_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8634"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XW4Ul7BL9EG"
      },
      "source": [
        "def XGB_t():\n",
        "    return Pipeline([\n",
        "        ('tfidf_vec', TfidfVectorizer()),  \n",
        "        ('XGB', xgb.XGBClassifier(n_estimators=500, max_depth=6, learning_rate=0.1,\n",
        "                                  subsample=.7, colsample_bytree=0.6, gamma=0.05))\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFf3NjZ8L9EG",
        "outputId": "655131a6-262d-4fc8-b011-aca0a5a8fa26"
      },
      "source": [
        "xgbt_clf = XGB_t()\n",
        "xgbt_clf.fit(X1_train, y1_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidf_vec',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_patte...\n",
              "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
              "                               colsample_bylevel=1, colsample_bynode=1,\n",
              "                               colsample_bytree=0.6, gamma=0.05,\n",
              "                               learning_rate=0.1, max_delta_step=0, max_depth=6,\n",
              "                               min_child_weight=1, missing=None,\n",
              "                               n_estimators=500, n_jobs=1, nthread=None,\n",
              "                               objective='binary:logistic', random_state=0,\n",
              "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
              "                               seed=None, silent=None, subsample=0.7,\n",
              "                               verbosity=1))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGBYaEohL9EG",
        "outputId": "c0b53e3f-466a-4ee5-f1e2-bbf30e676eb7"
      },
      "source": [
        "xgbt_clf.score(X1_test, y1_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8638"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7k4NjQjL9EI"
      },
      "source": [
        "### Voting Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sQ3hJNGL9EI"
      },
      "source": [
        "from sklearn.ensemble import VotingClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01HviSRtL9EI"
      },
      "source": [
        "voting_clf = VotingClassifier(estimators=[\n",
        "    (\"mnb_clf\", MNB_tfidf_Classifier()),\n",
        "    (\"log_clf\", LogisticRegression_c()),\n",
        "#     (\"rfc_clf\", RFC_c()),\n",
        "    (\"etc_clf\", ETC_c()),\n",
        "    (\"xgb_clf\", XGB_t()),\n",
        "#     (\"lgb_clf\", LGB_t())\n",
        "], voting='soft')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n___jp9HL9EJ",
        "outputId": "c00f8419-5f08-4f11-97f9-93d76aee006d"
      },
      "source": [
        "voting_clf.fit(X1_train, y1_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/Users/pan/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VotingClassifier(estimators=[('mnb_clf',\n",
              "                              Pipeline(memory=None,\n",
              "                                       steps=[('tfidf_vec',\n",
              "                                               TfidfVectorizer(analyzer='word',\n",
              "                                                               binary=False,\n",
              "                                                               decode_error='strict',\n",
              "                                                               dtype=<class 'numpy.float64'>,\n",
              "                                                               encoding='utf-8',\n",
              "                                                               input='content',\n",
              "                                                               lowercase=True,\n",
              "                                                               max_df=1.0,\n",
              "                                                               max_features=None,\n",
              "                                                               min_df=1,\n",
              "                                                               ngram_range=(1,\n",
              "                                                                            1),\n",
              "                                                               norm='l2',\n",
              "                                                               preprocessor=None,\n",
              "                                                               smooth_idf=True,\n",
              "                                                               stop_words=None,\n",
              "                                                               strip_acc...\n",
              "                                                             colsample_bytree=0.6,\n",
              "                                                             gamma=0.05,\n",
              "                                                             learning_rate=0.1,\n",
              "                                                             max_delta_step=0,\n",
              "                                                             max_depth=6,\n",
              "                                                             min_child_weight=1,\n",
              "                                                             missing=None,\n",
              "                                                             n_estimators=500,\n",
              "                                                             n_jobs=1,\n",
              "                                                             nthread=None,\n",
              "                                                             objective='binary:logistic',\n",
              "                                                             random_state=0,\n",
              "                                                             reg_alpha=0,\n",
              "                                                             reg_lambda=1,\n",
              "                                                             scale_pos_weight=1,\n",
              "                                                             seed=None,\n",
              "                                                             silent=None,\n",
              "                                                             subsample=0.7,\n",
              "                                                             verbosity=1))],\n",
              "                                       verbose=False))],\n",
              "                 flatten_transform=True, n_jobs=None, voting='soft',\n",
              "                 weights=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xtFNfmmL9EJ",
        "outputId": "5d820175-2645-407e-e1c1-94fc037e675e"
      },
      "source": [
        "voting_clf.score(X1_test, y1_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.89"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RH1O5XiaL9EJ"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrDh0qslL9EJ"
      },
      "source": [
        "y_predict = voting_clf.predict(X1_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FSBLjROL9EJ",
        "outputId": "1011d9d1-ad20-4ea9-f289-eb0ea1e2819e"
      },
      "source": [
        "roc_auc_score(y1_test, y_predict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8899324331244112"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWo8QnusL9EK"
      },
      "source": [
        "# Part 3 Using Paddlepaddle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbHlJdh_L9EK"
      },
      "source": [
        "from __future__ import print_function\n",
        "import paddle\n",
        "import paddle.fluid as fluid\n",
        "import numpy as np\n",
        "import sys\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5o5KxjSIL9EK"
      },
      "source": [
        "## Model Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wx-dZb8oL9EK"
      },
      "source": [
        "CLASS_DIM = 2   #Number of categories for sentiment analysis\n",
        "EMB_DIM = 128     \n",
        "HID_DIM = 512  #Dimensions of hide layer   \n",
        "STACKED_NUM = 3   #LSTM Layers of the bidirectional stack\n",
        "BATCH_SIZE = 128  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eit6FIlPL9EK"
      },
      "source": [
        "## Text Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZwi41uDL9EK"
      },
      "source": [
        "def convolution_net(data, input_dim, class_dim, emb_dim, hid_dim):\n",
        "    \n",
        "    emb = fluid.layers.embedding(\n",
        "        input=data, size=[input_dim, emb_dim], is_sparse=True)\n",
        "    \n",
        "    #fluid.nets.sequence_conv_pool contains both convolution and pooling layers.\n",
        "    conv_3 = fluid.nets.sequence_conv_pool(\n",
        "        input=emb,\n",
        "        num_filters=hid_dim,\n",
        "        filter_size=3,\n",
        "        act=\"tanh\",\n",
        "        pool_type=\"sqrt\")\n",
        "    conv_4 = fluid.nets.sequence_conv_pool(\n",
        "        input=emb,\n",
        "        num_filters=hid_dim,\n",
        "        filter_size=4,\n",
        "        act=\"tanh\",\n",
        "        pool_type=\"sqrt\")\n",
        "    prediction = fluid.layers.fc(\n",
        "        input=[conv_3, conv_4], size=class_dim, act=\"softmax\")\n",
        "    return prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efkaPmc9L9EL"
      },
      "source": [
        "## Stacked bidirectional LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tadTSEv2L9EL"
      },
      "source": [
        "def stacked_lstm_net(data, input_dim, class_dim, emb_dim, hid_dim, stacked_num):\n",
        "\n",
        "    # Calculate word vector\n",
        "    emb = fluid.layers.embedding(\n",
        "        input=data, size=[input_dim, emb_dim], is_sparse=True)\n",
        "    \n",
        "    #First stack\n",
        "    #Fully connected layer\n",
        "    fc1 = fluid.layers.fc(input=emb, size=hid_dim)\n",
        "    \n",
        "    #lstm layer\n",
        "    lstm1, cell1 = fluid.layers.dynamic_lstm(input=fc1, size=hid_dim)\n",
        "\n",
        "    inputs = [fc1, lstm1]\n",
        "    \n",
        "    #All remaining stack structures\n",
        "    for i in range(2, stacked_num + 1):\n",
        "        fc = fluid.layers.fc(input=inputs, size=hid_dim)\n",
        "        lstm, cell = fluid.layers.dynamic_lstm(\n",
        "            input=fc, size=hid_dim, is_reverse=(i % 2) == 0)\n",
        "        inputs = [fc, lstm]\n",
        "    \n",
        "    #pooling layer\n",
        "    fc_last = fluid.layers.sequence_pool(input=inputs[0], pool_type='max')\n",
        "    lstm_last = fluid.layers.sequence_pool(input=inputs[1], pool_type='max')\n",
        "    \n",
        "    #Fully connected layer, softmax prediction\n",
        "    prediction = fluid.layers.fc(\n",
        "        input=[fc_last, lstm_last], size=class_dim, act='softmax')\n",
        "    return prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARRKr0ZUL9EL"
      },
      "source": [
        "def inference_program(word_dict):\n",
        "    data = fluid.layers.data(\n",
        "        name=\"words\", shape=[1], dtype=\"int64\", lod_level=1)\n",
        "\n",
        "    dict_dim = len(word_dict)\n",
        "    net = convolution_net(data, dict_dim, CLASS_DIM, EMB_DIM, HID_DIM)\n",
        "    #net = stacked_lstm_net(data, dict_dim, CLASS_DIM, EMB_DIM, HID_DIM, STACKED_NUM)\n",
        "    return net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJFawp0BL9EL"
      },
      "source": [
        "def train_program(prediction):\n",
        "    label = fluid.layers.data(name=\"label\", shape=[1], dtype=\"int64\")\n",
        "    cost = fluid.layers.cross_entropy(input=prediction, label=label)\n",
        "    avg_cost = fluid.layers.mean(cost)\n",
        "    accuracy = fluid.layers.accuracy(input=prediction, label=label)\n",
        "    return [avg_cost, accuracy]   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EXQe2AsL9EL"
      },
      "source": [
        "def optimizer_func():\n",
        "    return fluid.optimizer.Adagrad(learning_rate=0.002)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pAI3iJuL9EL"
      },
      "source": [
        "use_cuda = False  \n",
        "place = fluid.CUDAPlace(0) if use_cuda else fluid.CPUPlace()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-4j-X7nL9EM",
        "outputId": "60414833-77ef-4f36-b176-2b6209bec8a5"
      },
      "source": [
        "print(\"Loading IMDB word dict....\")\n",
        "word_dict = paddle.dataset.imdb.word_dict()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading IMDB word dict....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[==================================================]b%2FaclImdb_v1.tar.gz not found, downloading https://dataset.bj.bcebos.com/imdb%2FaclImdb_v1.tar.gz\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPyZpi6CL9EM",
        "outputId": "7e84e485-4bbc-450a-c446-81b682863597"
      },
      "source": [
        "print (\"Reading training data....\")\n",
        "train_reader = paddle.batch(\n",
        "    paddle.reader.shuffle(\n",
        "        paddle.dataset.imdb.train(word_dict), buf_size=25000),\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading training data....\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4aZIV4jL9EM",
        "outputId": "f73b40e5-9249-4439-a3ca-f9120f567b71"
      },
      "source": [
        "print(\"Reading testing data....\")\n",
        "test_reader = paddle.batch(\n",
        "    paddle.dataset.imdb.test(word_dict), batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading testing data....\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWbzUkzjL9EM"
      },
      "source": [
        "exe = fluid.Executor(place)\n",
        "prediction = inference_program(word_dict)\n",
        "[avg_cost, accuracy] = train_program(prediction)\n",
        "sgd_optimizer = optimizer_func()\n",
        "sgd_optimizer.minimize(avg_cost)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcqAFXTRL9EN"
      },
      "source": [
        "## Calculate the result of the model on the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6SijRCXL9EN"
      },
      "source": [
        "def train_test(program, reader):\n",
        "    count = 0\n",
        "    feed_var_list = [\n",
        "        program.global_block().var(var_name) for var_name in feed_order\n",
        "    ]\n",
        "    feeder_test = fluid.DataFeeder(feed_list=feed_var_list, place=place)\n",
        "    test_exe = fluid.Executor(place)\n",
        "    accumulated = len([avg_cost, accuracy]) * [0]\n",
        "    for test_data in reader():\n",
        "        avg_cost_np = test_exe.run(\n",
        "            program=program,\n",
        "            feed=feeder_test.feed(test_data),\n",
        "            fetch_list=[avg_cost, accuracy])\n",
        "        accumulated = [\n",
        "            x[0] + x[1][0] for x in zip(accumulated, avg_cost_np)\n",
        "        ]\n",
        "        count += 1\n",
        "    return [x / count for x in accumulated]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QcRUZfxL9EN"
      },
      "source": [
        "params_dirname = \"understand_sentiment_conv.inference.model\"\n",
        "feed_order = ['words', 'label']\n",
        "pass_num = 1  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrkKFXe3L9EN"
      },
      "source": [
        "def train_loop(main_program):  \n",
        "    exe.run(fluid.default_startup_program())\n",
        "    feed_var_list_loop = [\n",
        "        main_program.global_block().var(var_name) for var_name in feed_order\n",
        "    ]\n",
        "    feeder = fluid.DataFeeder(\n",
        "        feed_list=feed_var_list_loop, place=place)\n",
        "\n",
        "    test_program = fluid.default_main_program().clone(for_test=True)\n",
        "    \n",
        "    # Training loop\n",
        "    for epoch_id in range(pass_num):\n",
        "        for step_id, data in enumerate(train_reader()):\n",
        "            metrics = exe.run(main_program,\n",
        "                              feed=feeder.feed(data),\n",
        "                              fetch_list=[avg_cost, accuracy])\n",
        "         \n",
        "            # Testing Results\n",
        "            avg_cost_test, acc_test = train_test(test_program, test_reader)\n",
        "            print('Step {0}, Test Loss {1:0.2}, Acc {2:0.2}'.format(\n",
        "                step_id, avg_cost_test, acc_test))\n",
        "\n",
        "            print(\"Step {0}, Epoch {1} Metrics {2}\".format(\n",
        "                step_id, epoch_id, list(map(np.array,\n",
        "                                            metrics))))\n",
        "            if step_id == 30:\n",
        "                if params_dirname is not None:\n",
        "                    fluid.io.save_inference_model(params_dirname, [\"words\"],\n",
        "                                                  prediction, exe)\n",
        "                return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "0ao-PZFgL9EN",
        "outputId": "18de21df-2467-40b9-a2ba-cc7520d7434c"
      },
      "source": [
        "train_loop(fluid.default_main_program())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 0, Test Loss 2.1, Acc 0.5\n",
            "Step 0, Epoch 0 Metrics [array([0.697872], dtype=float32), array([0.4609375], dtype=float32)]\n",
            "Step 1, Test Loss 0.74, Acc 0.5\n",
            "Step 1, Epoch 0 Metrics [array([2.253068], dtype=float32), array([0.453125], dtype=float32)]\n",
            "Step 2, Test Loss 0.77, Acc 0.5\n",
            "Step 2, Epoch 0 Metrics [array([0.7364794], dtype=float32), array([0.4921875], dtype=float32)]\n",
            "Step 3, Test Loss 0.75, Acc 0.51\n",
            "Step 3, Epoch 0 Metrics [array([0.7874141], dtype=float32), array([0.4921875], dtype=float32)]\n",
            "Step 4, Test Loss 0.74, Acc 0.5\n",
            "Step 4, Epoch 0 Metrics [array([0.79995894], dtype=float32), array([0.453125], dtype=float32)]\n",
            "Step 5, Test Loss 0.65, Acc 0.63\n",
            "Step 5, Epoch 0 Metrics [array([0.70945644], dtype=float32), array([0.53125], dtype=float32)]\n",
            "Step 6, Test Loss 0.65, Acc 0.59\n",
            "Step 6, Epoch 0 Metrics [array([0.64539003], dtype=float32), array([0.65625], dtype=float32)]\n",
            "Step 7, Test Loss 0.65, Acc 0.65\n",
            "Step 7, Epoch 0 Metrics [array([0.65699106], dtype=float32), array([0.6171875], dtype=float32)]\n",
            "Step 8, Test Loss 0.63, Acc 0.64\n",
            "Step 8, Epoch 0 Metrics [array([0.6213407], dtype=float32), array([0.640625], dtype=float32)]\n",
            "Step 9, Test Loss 0.62, Acc 0.68\n",
            "Step 9, Epoch 0 Metrics [array([0.60490876], dtype=float32), array([0.7265625], dtype=float32)]\n",
            "Step 10, Test Loss 0.61, Acc 0.67\n",
            "Step 10, Epoch 0 Metrics [array([0.6059818], dtype=float32), array([0.734375], dtype=float32)]\n",
            "Step 11, Test Loss 0.61, Acc 0.64\n",
            "Step 11, Epoch 0 Metrics [array([0.62263006], dtype=float32), array([0.640625], dtype=float32)]\n",
            "Step 12, Test Loss 0.61, Acc 0.7\n",
            "Step 12, Epoch 0 Metrics [array([0.6381704], dtype=float32), array([0.6015625], dtype=float32)]\n",
            "Step 13, Test Loss 0.62, Acc 0.61\n",
            "Step 13, Epoch 0 Metrics [array([0.60530543], dtype=float32), array([0.734375], dtype=float32)]\n",
            "Step 14, Test Loss 0.66, Acc 0.62\n",
            "Step 14, Epoch 0 Metrics [array([0.62055254], dtype=float32), array([0.6328125], dtype=float32)]\n",
            "Step 15, Test Loss 0.56, Acc 0.75\n",
            "Step 15, Epoch 0 Metrics [array([0.58520037], dtype=float32), array([0.6640625], dtype=float32)]\n",
            "Step 16, Test Loss 0.55, Acc 0.78\n",
            "Step 16, Epoch 0 Metrics [array([0.6182339], dtype=float32), array([0.6640625], dtype=float32)]\n",
            "Step 17, Test Loss 0.55, Acc 0.76\n",
            "Step 17, Epoch 0 Metrics [array([0.5579487], dtype=float32), array([0.765625], dtype=float32)]\n",
            "Step 18, Test Loss 0.56, Acc 0.7\n",
            "Step 18, Epoch 0 Metrics [array([0.57464874], dtype=float32), array([0.7265625], dtype=float32)]\n",
            "Step 19, Test Loss 0.54, Acc 0.76\n",
            "Step 19, Epoch 0 Metrics [array([0.52074516], dtype=float32), array([0.7578125], dtype=float32)]\n",
            "Step 20, Test Loss 0.53, Acc 0.75\n",
            "Step 20, Epoch 0 Metrics [array([0.5255684], dtype=float32), array([0.7734375], dtype=float32)]\n",
            "Step 21, Test Loss 0.54, Acc 0.74\n",
            "Step 21, Epoch 0 Metrics [array([0.53034353], dtype=float32), array([0.734375], dtype=float32)]\n",
            "Step 22, Test Loss 0.54, Acc 0.72\n",
            "Step 22, Epoch 0 Metrics [array([0.55724704], dtype=float32), array([0.7265625], dtype=float32)]\n",
            "Step 23, Test Loss 0.54, Acc 0.73\n",
            "Step 23, Epoch 0 Metrics [array([0.60450304], dtype=float32), array([0.6796875], dtype=float32)]\n",
            "Step 24, Test Loss 0.49, Acc 0.8\n",
            "Step 24, Epoch 0 Metrics [array([0.5393014], dtype=float32), array([0.75], dtype=float32)]\n",
            "Step 25, Test Loss 0.47, Acc 0.82\n",
            "Step 25, Epoch 0 Metrics [array([0.4819855], dtype=float32), array([0.8125], dtype=float32)]\n",
            "Step 26, Test Loss 0.47, Acc 0.8\n",
            "Step 26, Epoch 0 Metrics [array([0.455698], dtype=float32), array([0.8125], dtype=float32)]\n",
            "Step 27, Test Loss 0.47, Acc 0.8\n",
            "Step 27, Epoch 0 Metrics [array([0.46263516], dtype=float32), array([0.7734375], dtype=float32)]\n",
            "Step 28, Test Loss 0.46, Acc 0.81\n",
            "Step 28, Epoch 0 Metrics [array([0.4480282], dtype=float32), array([0.8125], dtype=float32)]\n",
            "Step 29, Test Loss 0.44, Acc 0.83\n",
            "Step 29, Epoch 0 Metrics [array([0.47295645], dtype=float32), array([0.75], dtype=float32)]\n",
            "Step 30, Test Loss 0.44, Acc 0.83\n",
            "Step 30, Epoch 0 Metrics [array([0.39714667], dtype=float32), array([0.9140625], dtype=float32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yE5XEHCeL9EN"
      },
      "source": [
        "## Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAyvL90dL9EO"
      },
      "source": [
        "place = fluid.CUDAPlace(0) if use_cuda else fluid.CPUPlace()\n",
        "exe = fluid.Executor(place)\n",
        "inference_scope = fluid.core.Scope()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlCZYqc_L9EO"
      },
      "source": [
        "reviews_str = [\n",
        "    'It would be stupid to not watch this', 'this is a great movie', 'this is very bad'\n",
        "]\n",
        "reviews = [c.split() for c in reviews_str]\n",
        "\n",
        "UNK = word_dict['<unk>']\n",
        "lod = []\n",
        "for c in reviews:\n",
        "    lod.append([word_dict.get(words, UNK) for words in c])\n",
        "\n",
        "base_shape = [[len(c) for c in lod]]\n",
        "\n",
        "tensor_words = fluid.create_lod_tensor(lod, base_shape, place)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcOKy5VtL9EO",
        "outputId": "aa5bbf4c-b131-405e-daa7-a37cae52c125"
      },
      "source": [
        "with fluid.scope_guard(inference_scope):\n",
        "\n",
        "    [inferencer, feed_target_names,\n",
        "     fetch_targets] = fluid.io.load_inference_model(params_dirname, exe)\n",
        "\n",
        "    assert feed_target_names[0] == \"words\"\n",
        "    results = exe.run(inferencer,\n",
        "                      feed={feed_target_names[0]: tensor_words},\n",
        "                      fetch_list=fetch_targets,\n",
        "                      return_numpy=False)\n",
        "    np_data = np.array(results[0])\n",
        "    for i, r in enumerate(np_data):\n",
        "        print(\"Predict probability of \", r[0], \" to be positive and \", r[1],\n",
        "              \" to be negative for review \\'\", reviews_str[i], \"\\'\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predict probability of  0.6173885  to be positive and  0.38261148  to be negative for review ' read the book forget the movie '\n",
            "Predict probability of  0.6034464  to be positive and  0.39655355  to be negative for review ' this is a great movie '\n",
            "Predict probability of  0.58729345  to be positive and  0.41270655  to be negative for review ' this is very bad '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHDZJD4EL9EO"
      },
      "source": [
        "#### Convolutional neural networks excel at learning the spatial structure in input data.\n",
        "\n",
        "#### The IMDB review data does have a one-dimensional spatial structure in the sequence of words in reviews and the CNN may be able to pick out invariant features for good and bad sentiment. This learned spatial features may then be learned as sequences by an LSTM layer.\n",
        "\n",
        "#### We can easily add a one-dimensional CNN and max pooling layers after the Embedding layer which then feed the consolidated features to the LSTM. We can use a smallish set of 32 features with a small filter length of 3. The pooling layer can use the standard length of 2 to halve the feature map size."
      ]
    }
  ]
}